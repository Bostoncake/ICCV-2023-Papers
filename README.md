# ICCV-2023-Papers

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)
![Version](https://img.shields.io/badge/version-v0.0.0-rc0)
![GitHub repo size](https://img.shields.io/github/repo-size/DmitryRyumin/ICCV-2023-Papers)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/LICENSE)
[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/README.md)
![GitHub contributors](https://img.shields.io/github/contributors/dmitryryumin/ICCV-2023-Papers)
![GitHub commit activity (branch)](https://img.shields.io/github/commit-activity/t/dmitryryumin/ICCV-2023-Papers)
![GitHub closed issues](https://img.shields.io/github/issues-closed/DmitryRyumin/ICCV-2023-Papers)
![GitHub issues](https://img.shields.io/github/issues/DmitryRyumin/ICCV-2023-Papers)
![GitHub closed pull requests](https://img.shields.io/github/issues-pr-closed/DmitryRyumin/ICCV-2023-Papers)
![GitHub pull requests](https://img.shields.io/github/issues-pr/dmitryryumin/ICCV-2023-Papers)
![GitHub last commit](https://img.shields.io/github/last-commit/DmitryRyumin/ICCV-2023-Papers)
![GitHub watchers](https://img.shields.io/github/watchers/dmitryryumin/ICCV-2023-Papers)
![GitHub forks](https://img.shields.io/github/forks/dmitryryumin/ICCV-2023-Papers)
![GitHub Repo stars](https://img.shields.io/github/stars/dmitryryumin/ICCV-2023-Papers)
![Visitors](https://api.visitorbadge.io/api/combined?path=https%3A%2F%2Fgithub.com%2FDmitryRyumin%2FICCV-2023-Papers&label=Visitors&countColor=%23263759&style=flat)

---

ICCV 2023 Papers: Explore a comprehensive collection of cutting-edge research papers presented at [*ICCV 2023*](https://iccv2023.thecvf.com/), the premier computer vision conference. Keep up to date with the latest advances in computer vision and deep learning. Code implementations included. :star: the repository for the development of visual intelligence!

<p align="center">
    <a href="https://iccv2023.thecvf.com/" target="_blank">
        <img width="600" src="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/images/ICCV2023-banner.jpg" alt="ICCV 2023">
    </a>
<p>

---

[*The online version of the ICCV 2023 Conference Programme*](https://iccv2023.thecvf.com/main.conference.program-107.php), comprises a list of all accepted full papers, their presentation order, as well as the designated presentation times.

---

<a href="https://github.com/DmitryRyumin/NewEraAI-Papers" style="float:left;">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/arrow_click_cursor_pointer.png" width="25" />
  Other collections of the best AI conferences
</a>

<br />
<br />

> :exclamation: Conference table will be up to date all the time.

<table>
    <tr>
        <td><strong>Conference</strong></td>
        <td colspan="1" align="center"><strong>Year</strong></td>
    </tr>
    <tr>
      <td colspan="2" align="center"><i>Computer Vision (CV)</i></td>
    </tr>
    <tr>
        <td>CVPR</td>
        <td><a href="https://github.com/DmitryRyumin/CVPR-2023-Papers" target="_blank">2023</a></td>
    </tr>
    <tr>
      <td colspan="2" align="center"><i>Speech (SP)</i></td>
    </tr>
    <tr>
        <td>ICASSP</td>
        <td><a href="https://github.com/DmitryRyumin/ICASSP-2023-Papers" target="_blank">2023</a></td>
    </tr>
    <tr>
        <td>INTERSPEECH</td>
        <td><a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-Papers" target="_blank">2023</a></td>
    </tr>
</table>

---

## Contributors

<a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/graphs/contributors">
  <img src="http://contributors.nn.ci/api?repo=DmitryRyumin/ICCV-2023-Papers" />
</a>

<br />
<br />

Contributions to improve the completeness of this list are greatly appreciated. If you come across any overlooked papers, please **feel free to [*create pull requests*](https://github.com/DmitryRyumin/ICCV-2023-Papers/pulls), [*open issues*](https://github.com/DmitryRyumin/ICCV-2023-Papers/issues) or contact me via [*email*](mailto:neweraairesearch@gmail.com)**. Your participation is crucial to making this repository even better.

---

## [Papers](https://iccv2023.thecvf.com/main.conference.program-107.php)

> :exclamation: Final paper links will be added post-conference.

<details open>
<summary>List of sections<a id="sections"></a></summary>

- [3D from Multi-View and Sensors](#3d-from-multi-view-and-sensors)
- [Adversarial Attack and Defense](#adversarial-attack-and-defense)
- [Vision and Robotics](#vision-and-robotics)
- [Vision and Graphics](#vision-and-graphics)
- [Segmentation, Grouping and Shape Analysis](#segmentation-grouping-and-shape-analysis)
- [Recognition: Categorization](#recognition-categorization)
- [Explainable AI for CV](#explainable-ai-for-cv)
- [Neural Generative Models](#neural-generative-models)
- [Vision and Language](#vision-and-language)
- [Vision, Graphics, and Robotics](#vision-graphics-and-robotics)
- [Privacy, Security, Fairness, and Explainability](#privacy-security-fairness-and-explainability)
- [Fairness, Privacy, Ethics, Social-good, Transparency, Accountability in Vision](#fairness-privacy-ethics-social-good-transparency-accountability-in-vision)
- [First Person (Egocentric) Vision](#first-person-egocentric-vision)
- [Deep Learning Architectures](#deep-learning-architectures)
- [Recognition: Detection](#recognition-detection)
- [Image and Video Synthesis](#image-and-video-synthesis)
- [Vision and Audio](#vision-and-audio)
- [Recognition, Segmentation, and Shape Analysis](#recognition-segmentation-and-shape-analysis)
- [Generative AI](#generative-ai)
- [Humans, 3D Modeling, and Driving](#humans-3d-modeling-and-driving)
- [Low-Level Vision and Theory](#low-level-vision-and-theory)
- [Navigation and Autonomous Driving](#navigation-and-autonomous-driving)
- [3D from a Single Image and Shape-from-X](#3d-from-a-single-image-and-shape-from-x)
- [Motion Estimation, Matching and Tracking](#motion-estimation-matching-and-tracking)
- [Action and Event Understanding](#action-and-event-understanding)
- [Computational Imaging](#computational-imaging)
- [Embodied Vision: Active Agents; Simulation](#embodied-vision-active-agents-simulation)
- [Recognition: Retrieval](#recognition-retrieval)
- [Transfer, Low-Shot, Continual, Long-Tail Learning](#transfer-low-shot-continual-long-tail-learning)
- [Low-Level and Physics-based Vision](#low-level-and-physics-based-vision)
- [Computer Vision Theory](#computer-vision-theory)
- [Video Analysis and Understanding](#video-analysis-and-understanding)
- [Object Pose Estimation and Tracking](#object-pose-estimation-and-tracking)
- [3D Shape Modeling and Processing](#3d-shape-modeling-and-processing)
- [Human Pose/Shape Estimation](#human-poseshape-estimation)
- [Transfer, Low-Shot, and Continual Learning](#transfer-low-shot-and-continual-learning)
- [Self-, Semi-, and Unsupervised Learning](#self--semi--and-unsupervised-learning)
- [Self-, Semi-, Meta-, Unsupervised Learning](#self--semi--meta--unsupervised-learning)
- [Photogrammetry and Remote Sensing](#photogrammetry-and-remote-sensing)
- [Efficient and Scalable Vision](#efficient-and-scalable-vision)
- [Machine Learning (other than Deep Learning)](#machine-learning-other-than-deep-learning)
- [Document Analysis and Understanding](#document-analysis-and-understanding)
- [Biometrics](#biometrics)
- [Datasets and Evaluation](#datasets-and-evaluation)
- [Faces and Gestures](#faces-and-gestures)
- [Medical and Biological Vision; Cell Microscopy](#medical-and-biological-vision-cell-microscopy)
- [Scene Analysis and Understanding](#scene-analysis-and-understanding)
- [Multimodal Learning](#multimodal-learning)
- [Human-in-the-Loop Computer Vision](#human-in-the-loop-computer-vision)
- [Image and Video Forensics](#image-and-video-forensics)
- [Geometric Deep Learning](#geometric-deep-learning)
- [Vision Applications and Systems](#vision-applications-and-systems)
- [Machine Learning and Dataset](#machine-learning-and-dataset)

</details>

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### 3D from Multi-View and Sensors

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Multi-Modal Neural Radiance Field for Monocular Dense SLAM with a Light-Weight ToF Sensor | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.14383-b31b1b.svg)](https://arxiv.org/abs/2308.14383) | :heavy_minus_sign: |
| ScanNet++: A High-Fidelity Dataset of 3D Indoor Scenes | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://cy94.github.io/scannetpp/) | [![arXiv](https://img.shields.io/badge/arXiv-2308.11417-b31b1b.svg)](https://arxiv.org/abs/2308.11417) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=E6P9e2r6M8I) |
| Translating Images to Road Network: A Non-Autoregressive Sequence-to-Sequence Approach | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Doppelgangers: Learning to Disambiguate Images of Similar Structures | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://doppelgangers-3d.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/RuojinCai/Doppelgangers)](https://github.com/RuojinCai/Doppelgangers) | :heavy_minus_sign: | :heavy_minus_sign: |
| EgoLoc: Revisiting 3D Object Localization from Egocentric Videos with Visual Queries | [![GitHub](https://img.shields.io/github/stars/Wayne-Mai/EgoLoc)](https://github.com/Wayne-Mai/EgoLoc) | [![arXiv](https://img.shields.io/badge/arXiv-2212.06969-b31b1b.svg)](https://arxiv.org/abs/2212.06969) | :heavy_minus_sign: |
| ClothPose: A Real-world Benchmark for Visual Analysis of Garment Pose via an Indirect Recording Solution | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| EMR-MSF: Self-Supervised Recurrent Monocular Scene Flow Exploiting Ego-Motion Rigidity | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| ENVIDR: Implicit Differentiable Renderer with Neural Environment Lighting | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://nexuslrf.github.io/ENVIDR/) <br /> [![GitHub](https://img.shields.io/github/stars/nexuslrf/ENVIDR)](https://github.com/nexuslrf/ENVIDR) | [![arXiv](https://img.shields.io/badge/arXiv-2303.13022-b31b1b.svg)](https://arxiv.org/abs/2303.13022) | [![Google Drive](https://img.shields.io/badge/Google%20Drive-4285F4?style=for-the-badge&logo=googledrive&logoColor=white)](https://drive.google.com/file/d/18kU-IWVxboCG8SCGgrBA5JHC0JIgPCS8/view?t=17s) |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Adversarial Attack and Defense

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Robust Mixture-of-Expert Training for Convolutional Neural Networks | [![GitHub](https://img.shields.io/github/stars/OPTML-Group/Robust-MoE-CNN)](https://github.com/OPTML-Group/Robust-MoE-CNN) | [![arXiv](https://img.shields.io/badge/arXiv-2308.10110-b31b1b.svg)](https://arxiv.org/abs/2308.10110) | :heavy_minus_sign: |
| Set-Level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-Training Models | [![GitHub](https://img.shields.io/github/stars/Zoky-2020/SGA)](https://github.com/Zoky-2020/SGA) | [![arXiv](https://img.shields.io/badge/arXiv-2307.14061-b31b1b.svg)](https://arxiv.org/abs/2307.14061) | :heavy_minus_sign: |
| CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning | [![GitHub](https://img.shields.io/github/stars/nishadsinghi/CleanCLIP)](https://github.com/nishadsinghi/CleanCLIP) | [![arXiv](https://img.shields.io/badge/arXiv-2303.03323-b31b1b.svg)](https://arxiv.org/abs/2303.03323) | :heavy_minus_sign: |
| CGBA: Curvature-Aware Geometric Black-Box Attack | [![GitHub](https://img.shields.io/github/stars/Farhamdur/CGBA)](https://github.com/Farhamdur/CGBA) | [![arXiv](https://img.shields.io/badge/arXiv-2308.03163-b31b1b.svg)](https://arxiv.org/abs/2308.03163) | :heavy_minus_sign: |
| Robust Evaluation of Diffusion-based Adversarial Purification | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.09051-b31b1b.svg)](https://arxiv.org/abs/2303.09051) | :heavy_minus_sign: |
| Advancing Example Exploitation can Alleviate Critical Challenges in Adversarial Training | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| The Victim and the Beneficiary: Exploiting a Poisoned Model to Train a Clean Model on Poisoned Data | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| TIJO: Trigger Inversion with Joint Optimization for Defending Multimodal Backdoored Models | [![GitHub](https://img.shields.io/github/stars/SRI-CSL/TIJO)](https://github.com/SRI-CSL/TIJO) | [![arXiv](https://img.shields.io/badge/arXiv-2308.03906-b31b1b.svg)](https://arxiv.org/abs/2308.03906) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Vision and Robotics

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Simoun: Synergizing Interactive Motion-Appearance Understanding for Vision-based Reinforcement Learning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Among Us: Adversarially Robust Collaborative Perception by Consensus | [![GitHub](https://img.shields.io/github/stars/coperception/ROBOSAC)](https://github.com/coperception/ROBOSAC) | [![arXiv](https://img.shields.io/badge/arXiv-2303.09495-b31b1b.svg)](https://arxiv.org/abs/2303.09495) | :heavy_minus_sign: |
| Walking Your LiDOG: A Journey Through Multiple Domains for LiDAR Semantic Segmentation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://saltoricristiano.github.io/lidog/) <br /> [![GitHub](https://img.shields.io/github/stars/saltoricristiano/LiDOG)](https://github.com/saltoricristiano/LiDOG) | [![arXiv](https://img.shields.io/badge/arXiv-2304.11705-b31b1b.svg)](https://arxiv.org/abs/2304.11705) | :heavy_minus_sign: |
| Stabilizing Visual Reinforcement Learning via Asymmetric Interactive Cooperation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| MAAL: Multimodality-Aware Autoencoder-based Affordance Learning for 3D Articulated Objects | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Rethinking Range View Representation for LiDAR Segmentation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.05367-b31b1b.svg)](https://arxiv.org/abs/2303.05367) | :heavy_minus_sign: |
| PourIt!: Weakly-Supervised Liquid Perception from a Single Image for Visual Closed-Loop Robotic Pouring | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://hetolin.github.io/PourIt/) <br /> [![GitHub](https://img.shields.io/github/stars/hetolin/PourIt)](https://github.com/hetolin/PourIt) | [![arXiv](https://img.shields.io/badge/arXiv-2307.11299-b31b1b.svg)](https://arxiv.org/abs/2307.11299) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=R5SpiV0658Q) |
| CROSSFIRE: Camera Relocalization On Self-Supervised Features from an Implicit Representation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.04869-b31b1b.svg)](https://arxiv.org/abs/2303.04869) | :heavy_minus_sign: |
| Environment Agnostic Representation for Visual Reinforcement Learning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Test-Time Personalizable Forecasting of 3D Human Poses | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| HM-ViT: Hetero-Modal Vehicle-to-Vehicle Cooperative Perception with Vision Transformer | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2304.10628-b31b1b.svg)](https://arxiv.org/abs/2304.10628) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Vision and Graphics

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Efficient Neural Supersampling on a Novel Gaming Dataset | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.01483-b31b1b.svg)](https://arxiv.org/abs/2308.01483) | :heavy_minus_sign: |
| Locally Stylized Neural Radiance Fields | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| NEMTO: Neural Environment Matting for Novel View and Relighting Synthesis of Transparent Objects | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.11963-b31b1b.svg)](https://arxiv.org/abs/2303.11963) | :heavy_minus_sign: |
| DDColor: Towards Photo-Realistic and Semantic-Aware Image Colorization via Dual Decoders | [![GitHub](https://img.shields.io/github/stars/piddnad/DDColor)](https://github.com/piddnad/DDColor) <br /> [![ModelScope](https://img.shields.io/badge/ModelScope-DDColor-614BFF.svg)](https://www.modelscope.cn/models/damo/cv_ddcolor_image-colorization/summary) | [![arXiv](https://img.shields.io/badge/arXiv-2212.11613-b31b1b.svg)](https://arxiv.org/abs/2212.11613) | :heavy_minus_sign: |
| IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zju3dv.github.io/intrinsic_nerf/) <br /> [![GitHub](https://img.shields.io/github/stars/zju3dv/IntrinsicNeRF)](https://github.com/zju3dv/IntrinsicNeRF) | [![arXiv](https://img.shields.io/badge/arXiv-2210.00647-b31b1b.svg)](https://arxiv.org/abs/2210.00647) | :heavy_minus_sign: |
| PARIS: Part-Level Reconstruction and Motion Analysis for Articulated Objects | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://3dlg-hcvc.github.io/paris/) <br /> [![GitHub](https://img.shields.io/github/stars/3dlg-hcvc/paris)](https://github.com/3dlg-hcvc/paris) | [![arXiv](https://img.shields.io/badge/arXiv-2308.07391-b31b1b.svg)](https://arxiv.org/abs/2308.07391) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=tDSrROPCgUc) |
| ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mingyuan-zhang.github.io/projects/ReMoDiffuse.html) <br /> [![GitHub](https://img.shields.io/github/stars/mingyuan-zhang/ReMoDiffuse)](https://github.com/mingyuan-zhang/ReMoDiffuse) | [![arXiv](https://img.shields.io/badge/arXiv-2304.01116-b31b1b.svg)](https://arxiv.org/abs/2304.01116) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wSddrIA_2p8) |
| DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ds-fusion.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/tmaham/DS-Fusion)](https://github.com/tmaham/DS-Fusion) <br /> [![Hugging Face](https://img.shields.io/badge/🤗-tmaham-FFD21F.svg)](https://huggingface.co/spaces/tmaham/DS-Fusion-Express) | [![arXiv](https://img.shields.io/badge/arXiv-2303.09604-b31b1b.svg)](https://arxiv.org/abs/2303.09604) | :heavy_minus_sign: |
| Dynamic Mesh-Aware Radiance Fields | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mesh-aware-rf.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/YilingQiao/DMRF)](https://github.com/YilingQiao/DMRF) | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://drive.google.com/file/d/1uXg76v0CNVxgrQfBHPR5SbxIMXyPLFfQ/view) | :heavy_minus_sign: |
| Neural Reconstruction of Relightable Human Model from Monocular Video | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Neural Microfacet Fields for Inverse Rendering | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://half-potato.gitlab.io/posts/nmf/) <br /> [![GitHub](https://img.shields.io/github/stars/half-potato/nmf)](https://github.com/half-potato/nmf) | [![arXiv](https://img.shields.io/badge/arXiv-2303.17806-b31b1b.svg)](https://arxiv.org/abs/2303.17806) | :heavy_minus_sign: |
| A Theory of Topological Derivatives for Inverse Rendering of Geometry | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ishit.github.io/td/) | [![arXiv](https://img.shields.io/badge/arXiv-2308.09865-b31b1b.svg)](https://arxiv.org/abs/2308.09865) | :heavy_minus_sign: |
| Vox-E: Text-Guided Voxel Editing of 3D Objects | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://tau-vailab.github.io/Vox-E/) <br /> [![GitHub](https://img.shields.io/github/stars/TAU-VAILab/Vox-E)](https://github.com/TAU-VAILab/Vox-E) | [![arXiv](https://img.shields.io/badge/arXiv-2303.12048-b31b1b.svg)](https://arxiv.org/abs/2303.12048) | :heavy_minus_sign: |
| StegaNeRF: Embedding Invisible Information within Neural Radiance Fields |  |  |  |
| GlobalMapper: Arbitrary-Shaped Urban Layout Generation |  |  |  |
| Urban Radiance Field Representation with Deformable Neural Mesh Primitives |  |  |  |
| End2End Multi-View Feature Matching with Differentiable Pose Optimization |  |  |  |
| Tree-Structured Shading Decomposition |  |  |  |
| Lens Parameter Estimation for Realistic Depth of Field Synthesis |  |  |  |
| AttT2M: Text-Driven Human Motion Generation with Multi-Perspective Attention Mechanism |  |  |  |
| Cross-Modal Latent Space Alignment for Image to Avatar Translation |  |  |  |
| Computationally Efficient Neural Image Compression with Shallow Decoders |  |  |  |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Segmentation, Grouping and Shape Analysis

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Recognition: Categorization

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Explainable AI for CV

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Neural Generative Models

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Vision and Language

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Vision, Graphics, and Robotics

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Privacy, Security, Fairness, and Explainability

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Fairness, Privacy, Ethics, Social-good, Transparency, Accountability in Vision

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### First Person (Egocentric) Vision

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Deep Learning Architectures

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Recognition: Detection

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Image and Video Synthesis

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Vision and Audio

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Recognition, Segmentation, and Shape Analysis

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Generative AI

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Humans, 3D Modeling, and Driving

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Low-Level Vision and Theory

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Navigation and Autonomous Driving

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### 3D from a Single Image and Shape-from-X

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Motion Estimation, Matching and Tracking

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Action and Event Understanding

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Computational Imaging

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Embodied Vision: Active Agents; Simulation

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Recognition: Retrieval

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Transfer, Low-Shot, Continual, Long-Tail Learning

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Low-Level and Physics-based Vision

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Computer Vision Theory

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Video Analysis and Understanding

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Object Pose Estimation and Tracking

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### 3D Shape Modeling and Processing

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Human Pose/Shape Estimation

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Transfer, Low-Shot, and Continual Learning

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Self-, Semi-, and Unsupervised Learning

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Self-, Semi-, Meta-, Unsupervised Learning

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Photogrammetry and Remote Sensing

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Efficient and Scalable Vision

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Machine Learning (other than Deep Learning)

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Document Analysis and Understanding

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Biometrics

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Datasets and Evaluation

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Faces and Gestures

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Medical and Biological Vision; Cell Microscopy

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Scene Analysis and Understanding

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Multimodal Learning

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Human-in-the-Loop Computer Vision

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Image and Video Forensics

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Geometric Deep Learning

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Vision Applications and Systems

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Machine Learning and Dataset

> Will soon be added

---

## Star History

<p align="center">
    <a href="https://star-history.com/#Dmitryryumin/ICCV-2023-Papers&Date" target="_blank">
        <img width="500" src="https://api.star-history.com/svg?repos=Dmitryryumin/ICCV-2023-Papers&type=Date" alt="Star History Chart">
    </a>
<p>
